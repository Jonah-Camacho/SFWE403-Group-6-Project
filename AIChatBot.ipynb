{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "273b93c0-5cce-49ce-a665-e6dad396e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, yaml, numpy as np, tiktoken\n",
    "import ollama\n",
    "import os, json\n",
    "import os, sys\n",
    "import psycopg\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee091c75-b5a3-4791-8778-be1779e29979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "EMBED_MODEL = \"nomic-embed-text\"\n",
    "LLM = \"gemma3:4b\"\n",
    "\n",
    "# -----------------------------\n",
    "# System Prompts (RAG-aware)\n",
    "# -----------------------------\n",
    "SYSTEM_ASSISTANT = \"\"\"\n",
    "You are the University of Arizona Software Engineering Degree Advisor chatbot.\n",
    "\n",
    "GOAL\n",
    "- Help prospective and current students understand UA Software Engineering options (e.g., BS/BA/BAS; online vs. in-person), admission requirements, transfer credit, prerequisite chains, curriculum maps, course sequencing, key policies, timelines, tuition/fees and aid, advising and contacts, and typical career outcomes.\n",
    "- Always answer using ONLY the provided CONTEXT (snippets from the local handbook/notes).\n",
    "- If a requested detail is not present in CONTEXT, say so briefly and suggest a next step (advising email/office, official catalog, or submitting an official transfer evaluation)—do NOT invent details.\n",
    "\n",
    "STYLE\n",
    "- Be concise and structured: short paragraphs and bullet points when helpful.\n",
    "- Where relevant, add a tiny “Next steps” section with 1–3 actionable items.\n",
    "- If you cite something from the context, reference the section title or heading in plain text (e.g., “See: ‘Admission Requirements’”)—no external links here.\n",
    "\n",
    "GUARDRAILS\n",
    "- Do not assume up-to-date tuition/policy dates if CONTEXT doesn’t include them—state that students should verify with the official UA sources.\n",
    "- If the user asks questions outside Software Engineering degree info (e.g., unrelated campus facts), answer briefly only if CONTEXT includes it; otherwise, say you don’t have it in the docs and suggest where to check.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_RETRIEVER = \"\"\"\n",
    "Given the user’s last message and (optionally) the previous assistant reply, create a focused retrieval query emphasizing:\n",
    "- program type (BS/BA/BAS/online/in-person), admissions, prerequisites, transfer, curriculum, sequencing, policies, timelines, costs/aid, advising, outcomes, student support.\n",
    "Return ONLY the refined retrieval query text. No extra commentary.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Local knowledge base\n",
    "# -----------------------------\n",
    "BASE_PATH = Path.home() / \"OneDrive/Desktop/SoftwareDocuments/ChatBot.md\"\n",
    "BASE_MD = Path(\"C:\\\\Users\\\\jmcam\\\\OneDrive\\\\Desktop\\\\SoftwareDocuments\\\\ChatBot.md\").read_text(encoding=\"utf-8\")\n",
    "doc_text = BASE_MD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c4d254-9d4f-4d7b-a2b5-dbe3dbc4d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Tokenizer utilities\n",
    "# -----------------------------\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "def tokens(s: str) -> int:\n",
    "    return len(enc.encode(s))\n",
    "\n",
    "def split_by_headings(md: str):\n",
    "    # Split on H2/H3 boundaries to keep sections semantically meaningful\n",
    "    parts = re.split(r\"(?m)^##\\s+|^###\\s+\", md)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def smart_chunk(md: str, max_tokens=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits the markdown into ~max_tokens chunks with token overlap to preserve context at boundaries.\n",
    "    \"\"\"\n",
    "    raw = split_by_headings(md)\n",
    "    chunks = []\n",
    "    for part in raw:\n",
    "        if tokens(part) <= max_tokens:\n",
    "            chunks.append(part)\n",
    "        else:\n",
    "            words = part.split()\n",
    "            cur, cur_tokens = [], 0\n",
    "            for w in words:\n",
    "                tw = tokens(w + \" \")\n",
    "                if cur_tokens + tw > max_tokens:\n",
    "                    chunks.append(\" \".join(cur))\n",
    "                    # add token-overlap\n",
    "                    back = enc.decode(enc.encode(\" \".join(cur))[-overlap:])\n",
    "                    cur = back.split() if back else []\n",
    "                    cur_tokens = tokens(\" \".join(cur))\n",
    "                cur.append(w)\n",
    "                cur_tokens += tw\n",
    "            if cur:\n",
    "                chunks.append(\" \".join(cur))\n",
    "    return chunks\n",
    "\n",
    "chunks = smart_chunk(doc_text, max_tokens=450, overlap=60)\n",
    "\n",
    "# -----------------------------\n",
    "# Embeddings (normalized for cosine)\n",
    "# -----------------------------\n",
    "def embed_batch(texts):\n",
    "    vecs = []\n",
    "    for t in texts:\n",
    "        e = ollama.embeddings(model=EMBED_MODEL, prompt=t)[\"embedding\"]\n",
    "        vecs.append(np.array(e, dtype=np.float32))\n",
    "    arr = np.vstack(vecs) if vecs else np.zeros((0,1), dtype=np.float32)\n",
    "    arr /= (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-9)\n",
    "    return arr\n",
    "\n",
    "chunk_vecs = embed_batch(chunks)\n",
    "\n",
    "def retrieve(query: str, k=5):\n",
    "    q = np.array(ollama.embeddings(model=EMBED_MODEL, prompt=query)[\"embedding\"], dtype=np.float32)\n",
    "    q /= (np.linalg.norm(q) + 1e-9)\n",
    "    sims = chunk_vecs @ q\n",
    "    idx = np.argsort(-sims)[:k]\n",
    "    return [(float(sims[i]), chunks[i]) for i in idx]\n",
    "\n",
    "# -----------------------------\n",
    "# Conversational helpers\n",
    "# -----------------------------\n",
    "def build_context_block(history, k_ctx=5):\n",
    "    \"\"\"\n",
    "    Uses a small LLM prompt to refine the retrieval query from the latest turn(s),\n",
    "    then pulls top-k chunks and assembles a context block.\n",
    "    \"\"\"\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"software engineering program info\")\n",
    "    last_assistant = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"assistant\"), \"\")\n",
    "\n",
    "    # Ask the LLM to craft a tight retrieval query\n",
    "    rq = ollama.chat(\n",
    "        model=LLM,\n",
    "        messages=[\n",
    "            {\"role\":\"system\", \"content\": SYSTEM_RETRIEVER},\n",
    "            {\"role\":\"user\", \"content\": f\"AssistantPrev: {last_assistant}\\nUser: {last_user}\"}\n",
    "        ]\n",
    "    )[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Fallback: if the retriever got too fancy/empty\n",
    "    retrieval_query = rq if rq else (last_assistant + \" \" + last_user).strip()\n",
    "\n",
    "    ctx = retrieve(retrieval_query, k=k_ctx)\n",
    "    context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "    return context_block\n",
    "\n",
    "def advisor_turn(history, new_session=False, k_ctx=5):\n",
    "    \"\"\"\n",
    "    Produces the advisor’s reply (answers the user). On a fresh session,\n",
    "    offers a brief welcome and asks what they need.\n",
    "    \"\"\"\n",
    "    if new_session or not any(m[\"role\"] == \"user\" for m in history):\n",
    "        # Start fresh: generic opening based on context that tends to be universally helpful\n",
    "        ctx = retrieve(\"overview of UA software engineering program, admissions, curriculum, advising, costs\", k=k_ctx)\n",
    "        context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_ASSISTANT},\n",
    "            {\"role\": \"user\", \"content\":\n",
    "                \"CONTEXT:\\n\"\n",
    "                f\"{context_block}\\n\\n\"\n",
    "                \"Start a friendly, concise welcome as the UA Software Engineering Degree Advisor. \"\n",
    "                \"Offer help with admissions, transfer credits, curriculum planning, timelines, and advising. \"\n",
    "                \"Ask what they’re looking for.\"}\n",
    "        ]\n",
    "        return ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Normal turn: answer the user using top-k retrieved chunks\n",
    "    context_block = build_context_block(history, k_ctx=k_ctx)\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"\")\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_ASSISTANT},\n",
    "        {\"role\":\"user\", \"content\":\n",
    "         \"CONTEXT:\\n\"\n",
    "         f\"{context_block}\\n\\n\"\n",
    "         f\"USER QUESTION:\\n{last_user}\\n\\n\"\n",
    "         \"TASK:\\n\"\n",
    "         \"- Answer directly and concisely using ONLY the CONTEXT above.\\n\"\n",
    "         \"- If a detail is not in CONTEXT, say so and suggest a concrete next step.\\n\"\n",
    "         \"- End with a short 'Next steps' list (1–3 bullets) when useful.\\n\"}\n",
    "    ]\n",
    "    return ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "def advisor_tip(history, k_ctx=3):\n",
    "    \"\"\"\n",
    "    Returns one short tip related to the user's latest question (<=25 words).\n",
    "    \"\"\"\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"software engineering\")\n",
    "    ctx = retrieve(last_user, k=k_ctx)\n",
    "    context_block = \"\\n\\n---\\n\".join([c for _, c in ctx]) if ctx else \"\"\n",
    "    messages = [\n",
    "        {\"role\":\"system\", \"content\": SYSTEM_ASSISTANT},\n",
    "        {\"role\":\"user\", \"content\":\n",
    "         f\"CONTEXT:\\n{context_block}\\n\"\n",
    "         f\"User asked: {last_user}\\nProvide ONE short tip (<=25 words).\"}\n",
    "    ]\n",
    "    return ollama.chat(model=LLM, messages=messages)[\"message\"][\"content\"].strip()\n",
    "\n",
    "def advisor_sources(history, k_ctx=5):\n",
    "    \"\"\"\n",
    "    Returns a compact list of section headings/keywords from the retrieved chunks\n",
    "    so the user can see what parts of the docs were used.\n",
    "    \"\"\"\n",
    "    last_user = next((m[\"content\"] for m in reversed(history) if m[\"role\"] == \"user\"), \"\")\n",
    "    ctx = retrieve(last_user or \"overview UA software engineering\", k=k_ctx)\n",
    "    # Heuristic: extract first line as a \"heading-ish\" clue\n",
    "    headings = []\n",
    "    for _, chunk in ctx:\n",
    "        first_line = chunk.splitlines()[0].strip()\n",
    "        # Trim very long lines\n",
    "        if len(first_line) > 120:\n",
    "            first_line = first_line[:117] + \"...\"\n",
    "        headings.append(f\"- {first_line}\")\n",
    "    return \"Sources (from your local doc):\\n\" + \"\\n\".join(headings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97ace824-d6b4-45f2-afa0-b0f03d3093b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# CLI Runner\n",
    "# -----------------------------\n",
    "def run_chat():\n",
    "    print(\"UA Software Engineering Advisor\\n\"\n",
    "          \"Commands: /start, /end, /tip, /sources\\n\")\n",
    "    history = []\n",
    "    started = False\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_in = input(\"> \").strip()\n",
    "\n",
    "            if user_in.lower() == \"/end\":\n",
    "                print(\"Session ended.\")\n",
    "                break\n",
    "\n",
    "            if user_in.lower() == \"/start\":\n",
    "                started = True\n",
    "                history = []\n",
    "                reply = advisor_turn(history, new_session=True)\n",
    "                print(f\"\\nAdvisor: {reply}\\n\")\n",
    "                history.append({\"role\":\"assistant\",\"content\":reply})\n",
    "                continue\n",
    "\n",
    "            if not started:\n",
    "                print('Type \"/start\" to begin.')\n",
    "                continue\n",
    "\n",
    "            if user_in.lower() == \"/tip\":\n",
    "                tip = advisor_tip(history)\n",
    "                print(f\"\\nTip: {tip}\\n\")\n",
    "                continue\n",
    "\n",
    "            if user_in.lower() == \"/sources\":\n",
    "                s = advisor_sources(history)\n",
    "                print(f\"\\n{s}\\n\")\n",
    "                continue\n",
    "\n",
    "            # User message\n",
    "            history.append({\"role\":\"user\",\"content\":user_in})\n",
    "            if len(history) > 24:\n",
    "                history = history[-24:]  # keep context compact\n",
    "\n",
    "            # Assistant reply\n",
    "            reply = advisor_turn(history)\n",
    "            print(f\"\\nAdvisor: {reply}\\n\")\n",
    "            history.append({\"role\":\"assistant\",\"content\":reply})\n",
    "\n",
    "        except (KeyboardInterrupt, EOFError):\n",
    "            print(\"\\nSession interrupted.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc5b34d7-6f61-4a25-af92-947db79772b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-14 09:53:48] Loading knowledge from: C:\\Users\\jmcam\\OneDrive\\Desktop\\SoftwareDocuments\\ChatBot.md\n",
      "UA Software Engineering Advisor\n",
      "Commands: /start, /end, /tip, /sources\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  /start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: Hello! Welcome – I’m your Software Engineering Degree Advisor here at the University of Arizona.\n",
      "\n",
      "I can help you with:\n",
      "\n",
      "*   Admissions\n",
      "*   Transfer Credits\n",
      "*   Curriculum Planning\n",
      "*   Timelines\n",
      "*   Advising\n",
      "\n",
      "What are you hoping to find out today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I was wondering what the typical timeline with classes looks like for a software engineering bachelors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advisor: I can answer questions about the typical timeline with classes for a Software Engineering bachelor’s degree at the University of Arizona.\n",
      "\n",
      "See: ‘Curriculum Maps’ for details on the course sequencing.\n",
      "\n",
      "Next steps:\n",
      "*   Review the Curriculum Maps for a detailed course sequence.\n",
      "*   Contact your academic advisor for personalized guidance.\n",
      "\n",
      "\n",
      "Session interrupted.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Entrypoint\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Loading knowledge from: {BASE_PATH}\")\n",
    "    run_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ea1536-776a-46b6-bb92-62da3aed34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: C:\\Users\\jmcam\\anaconda3\\python.exe\n",
      "Found git at: C:\\Program Files\\Git\\cmd\\git.exe\n",
      "git --version -> git version 2.51.0.windows.1\n",
      "C:\\Users\\jmcam\\OneDrive\\Desktop\\SoftwareDocuments\\SFWE403-Group-6-Project\n",
      "The system cannot find the file specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'AIChatBot.ipynb' did not match any files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "###################################################################################\n",
    "# Note: Run to commit the file to the git\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "import os, sys, subprocess, glob\n",
    "\n",
    "# Candidate install locations (system + per-user)\n",
    "cands = [\n",
    "    r\"C:\\Program Files\\Git\\cmd\\git.exe\",\n",
    "    r\"C:\\Program Files (x86)\\Git\\cmd\\git.exe\",\n",
    "    r\"%LOCALAPPDATA%\\Programs\\Git\\cmd\\git.exe\",\n",
    "]\n",
    "cands = [os.path.expandvars(p) for p in cands]\n",
    "\n",
    "# Also try to discover git.exe if installed in a nonstandard place\n",
    "cands += glob.glob(r\"C:\\Program Files\\Git\\cmd\\git.exe\")\n",
    "cands += glob.glob(os.path.expandvars(r\"%LOCALAPPDATA%\\Programs\\Git\\cmd\\git.exe\"))\n",
    "\n",
    "found = next((p for p in cands if os.path.exists(p)), None)\n",
    "print(\"Kernel:\", sys.executable)\n",
    "print(\"Found git at:\", found)\n",
    "\n",
    "if found:\n",
    "    git_cmd = os.path.dirname(found)\n",
    "    git_bin = git_cmd.replace(r\"\\cmd\", r\"\\bin\")\n",
    "    os.environ[\"PATH\"] += \";\" + git_cmd + \";\" + git_bin\n",
    "    try:\n",
    "        print(\"git --version ->\", subprocess.check_output([\"git\",\"--version\"], text=True).strip())\n",
    "    except Exception as e:\n",
    "        print(\"Tried to add PATH but still failing:\", e)\n",
    "else:\n",
    "    print(\"Could not locate git.exe automatically. If you know the path, run:\")\n",
    "    print(r'import os; os.environ[\"PATH\"] += r\";C:\\Path\\To\\Git\\cmd;C:\\Path\\To\\Git\\bin\"')\n",
    "\n",
    "\n",
    "SOURCE_FILE = r\"C:\\Users\\jmcam\\OneDrive\\Desktop\\SoftwareDocuments\\AIChatBot.ipynb\"\n",
    "DEST_DIR   = r\"C:\\Users\\jmcam\\OneDrive\\Desktop\\SoftwareDocuments\\SFWE403-Group-6-Project\"\n",
    "\n",
    "%cd \"{DEST_DIR}\"\n",
    "\n",
    "!git remote set-url origin https://github.com/Jonah-Camacho/SFWE403-Group-6-Project.git\n",
    "\n",
    "\n",
    "!copy /Y \"{SOURCE_FILE}\" \"AIChatBot.ipynb\"\n",
    "\n",
    "\n",
    "!git add -- \"AIChatBot.ipynb\"\n",
    "!git commit -m \"Update AIChatBot notebook\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf466b-0b26-4518-a433-7ffc07a437ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
